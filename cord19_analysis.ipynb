{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aefad6f",
   "metadata": {},
   "source": [
    "# CORD-19 Metadata: Exploration, Cleaning, Analysis, and Streamlit App\n",
    "\n",
    "This notebook walks through the full assignment: downloading `metadata.csv` from the CORD-19 dataset (or using a local copy), performing basic exploration, cleaning and preparation, analysis and visualizations, and generating a simple Streamlit app to explore the results.\n",
    "\n",
    "Notes: the `metadata.csv` file can be large. The notebook contains a `sample` mode that reads only the first N rows or uses `chunksize`. If you have limited RAM, use the sampling cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e1391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 0: Imports and helper functions\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc031617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: set a metadata URL or local path.\n",
    "# If you already downloaded metadata.csv, set METADATA_PATH to that file.\n",
    "# Otherwise, set METADATA_URL to download directly.\n",
    "METADATA_URL = None  # e.g. 'https://ai2-semanticscholar-cord-19.s3.amazonaws.com/2020-03-13/metadata.csv'\n",
    "METADATA_PATH = 'metadata.csv'  # local filename to save to / or load from\n",
    "# Use sample_mode to only load first N rows for quick iteration. Set to None to load full file.\n",
    "SAMPLE_MODE = 20000  # set to None to attempt full load; or an integer to read only that many rows\n",
    "CHUNKSIZE = 100000  # used for streaming/processing large files if needed\n",
    "print('Config: METADATA_PATH=', METADATA_PATH, 'SAMPLE_MODE=', SAMPLE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93264bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Download (optional) and load the data\n",
    "import urllib.request\n",
    "\n",
    ",\n",
    ",\n",
    "# If a URL is provided, try to download (wrapped for safety)\n",
    "if METADATA_URL and not os.path.exists(METADATA_PATH):\n",
    "    try:\n",
    "        download_metadata(METADATA_URL, METADATA_PATH)\n",
    "    except Exception as e:\n",
    "        print('Download failed:', e)\n",
    "\n",
    "# Loading function that supports sampling and chunked preview\n",
    "def load_metadata(path, sample_mode=None, chunksize=None):\n",
    "    if sample_mode is not None:\n",
    "        print(f'Loading first {sample_mode} rows from {path}...')\n",
    "        return pd.read_csv(path, nrows=sample_mode, dtype=str)\n",
    "    elif chunksize is not None:\n",
    "        print(f'Reading with chunksize={chunksize}; concatenating first 1 chunk for preview...')\n",
    "        it = pd.read_csv(path, chunksize=chunksize, dtype=str)\n",
    "        return next(it)\n",
    "    else:\n",
    "        print(f'Loading full file {path}...')\n",
    "        return pd.read_csv(path, dtype=str)\n",
    "\n",
    "# Load the data (this will error if the file does not exist locally)\n",
    "try:\n",
    "    df = load_metadata(METADATA_PATH, sample_mode=SAMPLE_MODE, chunksize=None)\n",
    "    print('Loaded DataFrame with', df.shape[0], 'rows and', df.shape[1], 'columns')\n",
    "except FileNotFoundError as e:\n",
    "    print('File not found. Please download metadata.csv into the notebook working directory or set METADATA_URL to a valid URL.')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look: first rows and data types\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame shape and info\n",
    "print('Shape:', df.shape)\n",
    "print('\n",
    "Info:')\n",
    "print(df.info())\n",
    "\n",
    "# Missing values per column (top columns)\n",
    "miss = df.isnull().sum().sort_values(ascending=False)\n",
    "miss.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce88e0",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning and Preparation\n",
    "\n",
    "We'll convert date columns, extract year, and compute an abstract word count. We will also identify columns with many missing values and create a cleaned subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert publish_time to datetime (safe conversion)\n",
    "if 'publish_time' in df.columns:\n",
    "    df['publish_time'] = pd.to_datetime(df['publish_time'], errors='coerce')\n",
    "    df['year'] = df['publish_time'].dt.year\n",
    "    print('publish_time converted; sample years:', df['year'].dropna().unique()[:10])\n",
    "else:\n",
    "    print('No publish_time column found in DataFrame.')\n",
    "\n",
    "# Create abstract_word_count if abstract column exists\n",
    "if 'abstract' in df.columns:\n",
    "    df['abstract_word_count'] = df['abstract'].fillna('').apply(lambda t: len(str(t).split()))\n",
    "    print('abstract_word_count created; summary:')\n",
    "    print(df['abstract_word_count'].describe())\n",
    "else:\n",
    "    print('No abstract column present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with many missing values\n",
    "total = len(df)\n",
    "missing_frac = (df.isnull().sum() / total).sort_values(ascending=False)\n",
    "missing_frac.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13006ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned subset: keep important columns and drop columns with >80% missing\n",
    "threshold = 0.8\n",
    "cols_keep = missing_frac[missing_frac <= threshold].index.tolist()\n",
    "print('Keeping', len(cols_keep), 'columns out of', len(df.columns))\n",
    "df_clean = df[cols_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053800a0",
   "metadata": {},
   "source": [
    "## Part 3: Analysis and Visualizations\n",
    "\n",
    "We'll compute: publications by year, top journals, word frequencies in titles, and create plots including a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f373b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications by year\n",
    "if 'year' in df_clean.columns:\n",
    "    year_counts = df_clean['year'].value_counts(dropna=True).sort_index()\n",
    "    print(year_counts.head())\n",
    "    plt.figure(figsize=(8,4))\n",
    "    year_counts.plot(kind='bar')\n",
    "    plt.title('Publications by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print('Year column not available for plotting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199cd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top journals / sources\n",
    "# CORD-19 sometimes uses 'journal' or 'journal' fields; try common column names\n",
    "journal_col = None\n",
    "for candidate in ['journal', 'journal_title', 'journal_ref', 'source_x', 'source_y']:\n",
    "    if candidate in df_clean.columns:\n",
    "        journal_col = candidate\n",
    "        break\n",
    "\n",
    "if journal_col:\n",
    "    top_journals = df_clean[journal_col].fillna('Unknown').value_counts().head(20)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(y=top_journals.index, x=top_journals.values, palette='viridis')\n",
    "    plt.title('Top publishing journals/sources')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Journal')\n",
    "    plt.tight_layout()\n",
    "    print('Using journal column:', journal_col)\n",
    "else:\n",
    "    print('No journal-like column found. Columns available:', df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7857130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in titles (simple tokenization)\n",
    "if 'title' in df_clean.columns:\n",
    "    titles = df_clean['title'].fillna('').astype(str).str.lower()\n",
    "    # basic cleaning and tokenization\n",
    "    def tokenize(s):\n",
    "        s = re.sub(r\n",
    ", ' ', s)\n",
    "        return [w for w in s.split() if len(w) > 2]\n",
    "    all_words = Counter()\n",
    "    for t in titles:\n",
    "        all_words.update(tokenize(t))\n",
    "    common_words = all_words.most_common(50)\n",
    "    common_words[:20]\n",
    "else:\n",
    "    print('No title column available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f94cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for titles (if pillow & wordcloud are installed)\n",
    "if 'title' in df_clean.columns:\n",
    "    text = ' '.join(df_clean['title'].dropna().astype(str).tolist())\n",
    "    wc = WordCloud(width=800, height=400, background_color='white').generate(text.lower())\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Titles')\n",
    "else:\n",
    "    print('Skipping word cloud: no title column')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0260ff",
   "metadata": {},
   "source": [
    "## Part 4: Save cleaned data and create Streamlit app file\n",
    "We'll save a cleaned CSV (`metadata_clean.csv`) and create a simple `app.py` that uses Streamlit to display the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fd2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data for the app (downsample if sample_mode used)\n",
    "out_clean = 'metadata_clean.csv'\n",
    "df_clean.to_csv(out_clean, index=False)\n",
    "print('Saved cleaned CSV to', out_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a73d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sample of cleaned data\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace6dd5",
   "metadata": {},
   "source": [
    "## Part 5: Notes / Next Steps and Reflection\n",
    "\n",
    "This notebook performed the requested tasks in a reproducible way and saved a cleaned CSV which the Streamlit app will use. See `README.md` for instructions to run the Streamlit app.\n",
    "\n",
    "Potential improvements: more robust title tokenization and stopword removal (use NLTK or spaCy), temporal smoothing of publication counts, interactive plots (Plotly), and additional metadata joins (PMC / PubMed IDs)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
